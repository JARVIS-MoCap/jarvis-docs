{"config":{"lang":["en"],"separator":"[\\s\\-\\.]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"JARVIS - Markerless 3D Motion Capture Toolbox","text":"<p>JARVIS makes highly precise markerless 3D motion capture easy. All you need to get started is a multi camera recording setup and an idea of what you want to track. Our Toolbox will assist you on every step along the way, from recording synchronised videos, to quickly and consistently annotating your data, all the way to the final 3D pose predictions. If you are interested in setting up a 3D Motion Capture System or just want to learn more about our toolbox we strongly recommend having a look at our Getting Started Guide and our Manual. Here you'll find an overview of our workflow as well as tutorials to help you build a successful 3D tracking pipeline with JARVIS.</p>"},{"location":"#why-jarvis","title":"Why JARVIS?","text":"<p>Markerlerss motion capture has become an essential data analysis tool in many fields of research, pioneered largely by DeepLabCut several years ago. While there have been significant improvements to those methods both in terms of capabilities and usability, those advances mainly tackle single image based 2D tracking. Setting up a system for multi camera 3D motion capture on the other hand still remains very challenging and time consuming.\\ JARVIS tries to change that by introducing an easy to use toolchain that was developed with highly precise 3D Tracking as the primary goal, not just an afterthought.</p> <p>Our System consists of three parts:</p> <ul> <li>The Acquisition Tool allows you to record synchronized high resolution videos from multiple cameras at high frame rates thanks to GPU accelerated online compression. If you use one of the recommended cameras you can get your system setup and running with just a handful of clicks.</li> <li>The Annotation Tool leverages the multi camera recordings by projecting your manual annotations on a subset of those cameras to the remaining ones, significantly reducing the amount of tedious manual annotations needed. It also provides easy-to-use interfaces for calibrating your cameras   and creating datasets.</li> <li>The HybridNet Pytorch Library is our state of the art 3D pose estimation architecture. The hybrid 2D/3D CNN architecture is faster to train and more precise than pure 3D CNNs and most importantly it can run on siginficantly smaller dataset sizes, saving you countless hours of mind numbing annotations while still achieving highly precise markerless motion capture - even in scenarios with heavy occlusion. </li> </ul> Human Hand Annotated Frames 10000  Number of subjects 4 Monkey Hand Annotated Frames 3000 Number of subjects 1 Rat Full Body Annotated Frames 2000 Number of subjects 1 Mouse Full Body Annotated Frames 3000 Number of subjects 3 <p></p> <p>Our model outperforms the other currently available methods when trained on our human hand tracking dataset. Below you can see the performance of our toolbox compared to the well established markerless tracking toolbox DeepLabCut (DLC) and the recently released Dannce 3D tracking toolbox:  </p> <p></p>"},{"location":"#supported-cameras","title":"Supported Cameras","text":"<p>As mentioned before to be able to take full advantage of our Acquisition Tool without any modifications you'll have to use one of our supported cameras. The table below shows all currently supported camera models, we'll try to expand this list in the near future!</p> Company Model Name FLIR Chameleon FLIR Blackfly S FLIR Grasshopper"},{"location":"downloads/downloads/","title":"Downloads","text":"<p>Here you can find the installers for the AnnotationTool and the AcquisitonTool for all currently supported operating systems.\\ Please checkout our Github Repositories if you want to build our tools from source.</p> AnnotationTool Windows 64 bit MacOS (Catalina or newer) Ubuntu 22.04 Ubuntu 20.04 Ubuntu 18.04 <p></p> AcquisitionTool Windows 64 bit Ubuntu 20.04 Ubuntu 18.04 Trigger Firmware <p></p>"},{"location":"getting_started/1_introduction/","title":"Getting Started with JARVIS","text":"<p>Welcome to our Getting Started Guide! This guide will teach you how to set up the three different parts of our toolbox and how to work with our example data. This will help you get an idea of all the steps required to build and use a 3D markerless motion capture setup. After working through this guide you will be ready to have a look at our Manual and learn all the things necessary to build you own motion capture system from scratch. Note that this Guide is somewhat back to front. You will learn how to work with annotated data before we show you how to actually annotate it.  One additional note, JARVIS uses two different formats for the annotated data. The one you will work with first is called the trainingset. A trainingset is a finished set of annotations in the right format to be used by the JARVIS Pytorch module. The second one is called a dataset, this is the format that is used by the AnnotationTool. We know this can be a bit confusing, but the two different formats are neccessary to make some features of JARVIS work.</p>"},{"location":"getting_started/2_exploring_example/","title":"Exploring the Provided Example Trainingset","text":"<p>Let's start by playing around with our provided example so you can familiarize with our software and get a better feel for the task and the workflow. The example data we're working with in this tutorial are recordings of one of our monkeys performing a simple grasping task in our 12 camera setup. Your task is to track his hand while he is enjoying a variety of fruits we hand him. We will split the task into four steps:</p> <ol> <li>Installing our Pytorch Toolbox and downloading the example recordings.</li> <li>Visualizing the provided annotations, both in 2D and 3D.</li> <li>Training the entire network stack.</li> <li>Predicting Poses for the Example Recording.</li> <li>Creating Annotated Videos from Your Predictions.</li> </ol>"},{"location":"getting_started/2_exploring_example/#1-installing-the-toolbox-and-downloading-the-data","title":"1. Installing the Toolbox and Downloading the Data","text":"<p>First let's take care of setting up the software. Make sure you have a version of Anaconda installed. If you want to train networks also make sure that your PC has a Nvidia GPU with working CUDA drivers installed. There are only a few simple steps you need to take to install the toolbox: - Download the python package. To do this open up a terminal and run: <pre><code>git clone https://github.com/JARVIS-MoCap/JARVIS-HybridNet.git &amp;&amp; cd JARVIS-HybridNet\n</code></pre> Alternatively you download it directly by clicking here.</p> <ul> <li> <p>Create the <code>jarvis</code> Anaconda environment by running: <pre><code>conda create -n jarvis python=3.9  pytorch=1.10.1 torchvision cudatoolkit=11.3 notebook  -c pytorch\n</code></pre></p> </li> <li> <p>Activate the environment (you will need to do this every time you open a terminal to use JARVIS): <pre><code>conda activate jarvis\n</code></pre></p> </li> <li> <p>Install the required version of the setuptools package: <pre><code>pip install -U setuptools==59.5.0\n</code></pre></p> </li> <li> <p>Install JARVIS: <pre><code>pip install -e .\n</code></pre></p> </li> </ul> <p>With that out of the way the only thing left to do is downloading the example recordings by clicking here.  Congratulations, you are all set up now! To launch our handy streamlit GUI interface just open a terminal, activate the conda environment by running <code>conda activate jarvis</code> and type <code>jarvis launch</code>.  Alternatively you can also interact with jarvis through the command line. To do this activate the conda environment and then run <code>jarvis launch-cli</code>. The following sections give you the option to switch between instructions for both methods by selecting the respective tabs. </p>"},{"location":"getting_started/2_exploring_example/#2-visualizing-the-example-trainingset","title":"2. Visualizing the Example Trainingset","text":"<p>Before we dive into training JARVIS to track anything it is always a good idea to have a look at the trainingset your are using, both in 2D and in 3D.</p> GUICLI <p>To do this using the streamlit dashboard first launch the JARVIS streamlit dashboard as described above by running <code>jarvis launch</code>. Once the GUI pops up in your browser you can select the Example_Project from the drop-down menu and then navigate to the visualization menu.</p> <p></p> <p>As you can see there are a bunch of option for visualizing both your predictions and your trainingset. You can see how that looks like above, but feel free to play around with it a bit to familiarize yourself with the data you are working with.</p> <p>To do this using the command line interface first launch it by running 'jarvis launch-cli'. You will see a menu appear in your terminal that you can navigate using your arrow keys. To visualize your dataset select the Visualize menu and then pick either the Dataset2D or the Dataset2D option.</p> <p></p> <p>To visualize the example trainingset select the 'Example_Project' and the 'Hand' skeleton preset. Other than that feel free to play around with the different options.You can cycle through all the available frames by pressing any key. Pressing 'q' or 'esc' will take you back to the Visualize menu.</p> <p>Once you start working with your own data, checking your trainingset before training is really important to ensure there was no problem when creating it and your network will get the input you expect it to get.</p>"},{"location":"getting_started/2_exploring_example/#3-training-the-entire-network","title":"3. Training the Entire Network","text":"<p>Now that you know what our data looks like it is time to train the network stack.</p> GUICLI <p>Using our GUI this is really easy, all you need to do is to navigate to the Train Full menu and press train as shown below. If everything works correctly you should see two progress bars as well as a plot showing the training progress appear. Depending on your GPU training might take up to a few hours, so a bit of patience is required at this point. If you don't want to wait you can also continue with our pretrained weights of course.</p> <p></p> <p>The CLI makes this very easy. All you need to do is launch the interface by running <code>jarvis launch-cli</code>, select the Train menu and then run Train all as shown below. If everything works correctly you should see a progress bar appearing. Depending on your GPU training might take up to a few hours, so a bit of patience is required at this point. If you don't want to wait you can also continue with our pretrained weights of course.</p> <p></p> More Info on Network Training <p>Our network stack is trained in four steps:</p> <ol> <li>Training CenterDetect: In this step a 2D-CNN is trained to detect the center of the entity you are tracking. This will be used to estimate the location of the entity in 3D, essentially telling the 3D-CNN where to look.</li> <li>Training KeypointDetect: In this step another 2D-CNN is trained to detect all your annotated keypoints in a single image. The output of this network will subsequently be used to construct the 3D feature volume that is the input of our 3D-CNN. </li> <li>Training HybridNet: In this step the 3D part of our full HybridNet architecture is trained. It's job is to use the 3D feature volumes created by the KeypointDetect stage to create the final 3D pose predictions.</li> </ol>"},{"location":"getting_started/2_exploring_example/#4-predicting-poses-for-the-example-recording","title":"4. Predicting Poses for the Example Recording","text":"<p>If you haven't already you should now download our example recording.</p> GUICLI <p>Once you have the example recording saved on your computer all you need to do is launch the JARVIS GUI and navigate to the Predict3D menu as shown below. Here you will have to specify a couple of things:</p> <ul> <li>Path of recording directory is the path of the example recording you just downloaded, it should include the 'Example_Recording' directory.</li> <li>Weights for CenterDetect / HybridNet lets you specify which weights you want to use. If you have trained models yourself you can leave them at 'latest'. If you didn't train the network yourself you will have to put the path of the pretrained weights here. They can be found in the 'pretrained' directory inside your 'JARVIS-Hybridnet' folder.</li> <li>Start Frame &amp; Number Frames let you select on which part of the recording you want to run the prediction. For quick results set 'Number of Frames' to 1000. To predict until the end of the recording set it to -1.</li> </ul> <p>Once all those settings are correct, press the  Predict button and wait for the progress bar to fill up as shown below.</p> <p></p> <p>Once you have the example recording saved on your computer all you need to do is launch the JARVIS CLI and select Predict3D in the Predict menu as shown below. Here you will have to specify a couple of things:</p> <ul> <li>The Recordings Path is the path of the example recording you just downloaded, it should include the 'Example_Recording' directory.</li> <li>Select No for using TensorRT acceleration for now. If you installed the optional TensorRT packages this lets speed up predictions using NVIDIAs TensorRT library. Compiling the TRT models takes quite some time though.</li> <li>If you have trained models yourself you can use the most recently saved weights. Otherwise you will have to specify the path of the pretrained weights for the CenterDetect and the HybridNet networks here. They can be found in the 'pretrained' directory inside your 'JARVIS-Hybridnet' folder.</li> <li>Select No when asked if you want to use a calibration that is not in the trainingset.</li> <li>To quickly get some results also select No when asked wether you want to predict for the whole video</li> <li>Start Frame &amp; Number of Frames let you select on which part of the recording you want to run the prediction. For quick results set 'Number of Frames' to 1000, to predict until the end of the recording set it to -1.</li> </ul> <p>After answering all the prompts you should see a progress bar filling up as shown below.</p> <p></p> <p>Once the process is finished you will find a directory with a current timestamp in the projects folder under predictions. That folder contains a 'data3D.csv' file that contains the 3D coordinates and their corresponding confidences for every point in time. The directory also contains a '.yaml' file that holds some information necessary for creating videos from your predictions.</p>"},{"location":"getting_started/2_exploring_example/#5-creating-annotated-videos-from-your-predictions","title":"5. Creating Annotated Videos from Your Predictions","text":"<p>The easiest way to check the quality of the predictions you just created is looking at annotated videos. For the 3D predictions those videos are created by projecting the 3D coordinates of the keypoints back into all available camera perspectives.  </p> GUICLI <p>In the GUI navigate to the Visualization menu as shown below. Here the right prediction directory should already be selected. If you want you can remove or add cameras from the list of cameras for which you want to create annotated videos. You can now click Create Video as shown below.  If everything is set correctly you should find a directory containing your freshly labeled videos in the project directory after the progress bar is filled up.</p> <p></p> <p>Navigate to the Visualize Menu after launching the JARVIS CLI. After selecting Create Videos 3D and the Example_Project you should be able to select the Predictions_3D directory that you created in the last step. If you want you can now select and deselect all the cameras that will be used to create your annotated videos. If everything is set correctly you should find a directory containing your freshly labeled videos in the project directory after the progress bar is filled up.</p> <p></p>"},{"location":"getting_started/3_creating_trainingset/","title":"Creating a New Trainingset from our Example Recordings","text":"<p>Now that you know what a trainingset looks like and how you can use it to train the network we will take a step back and cover the process of creating this trainingset from a multi-camera recording. Like before we will split this task into smaller steps:</p> <ol> <li>Installing the AnnotationTool.</li> <li>Extracting a dataset from the Example Recording.</li> <li>Calibrating the Cameras.</li> <li>Annotating a Frameset.</li> <li>Exporting the dataset as a trainingset.</li> </ol> <p>If everything goes according to plan you will end up with a trainingset very similar to the one you used in the previous part of the tutorial.</p>"},{"location":"getting_started/3_creating_trainingset/#1-installing-the-annotationtool","title":"1. Installing the AnnotationTool","text":"<p>If you are using Windows, MacOS or Ubuntu all you have to do is got to our downloads page and grab the installer for your OS. If you are running a different Linux distribution you will have to build the AnnotationTool yourself. There is a guide on how to do that on its GitHub page. Once you install and launch the tool you will be greeted with a home screen that looks like this:</p> <p></p>"},{"location":"getting_started/3_creating_trainingset/#2-extracting-a-dataset-from-the-example-recording","title":"2. Extracting a Dataset from the Example Recording","text":"<p>Clicking on the first item in the list on the homescreen will open up the dataset extraction menu. This will allow you to extract framesets from your recordings that you will subsequently annotate. For this tutorial we will stick to the basics and use the fastest and quickest way of extracting a handful of framesets. Definitely check out the relevant section in our Manual to learn all the details about how to create a dataset that is as representative of your entire recording as possible. First let's go through the options in the Configuration section. - New Dataset Name is the name of the dataset you're going to create. - New Dataset Path is the directory in which your new dataset will be saved. - Framesets to extract per Segment is the number of framesets the tool will create, you can leave it at the default of 10 for this example. - Sampling Method is the method the tool uses to decide which frames in your recording to use. kmeans is the method you should use for your real datasets, but for this tutorial we will use uniform to make the extraction process a bit quicker.</p> <p>Next up is the Recordings section. Just click Add Recording and navigate to the 'Example_Recording' directory and click Open. The last thing left to do is to tell the tool the names of all the Entities and Keypoints you want to annotate. Entities refers to the animal/object you are tracking. (Note: The AnnotationTool supports annotating multiple entities in one dataset, but the rest of our toolbox currently does not). For our example you can load the 'Hand' preset by clicking Load Preset.</p> <p></p> <p>If your dataset menu looks like shown above you can click Create to create a new dataset. A progress window should pop up and once it is finished you should find a dataset folder containing a '.yaml' config file as well as a directory containing extracted frames for each camera.</p>"},{"location":"getting_started/3_creating_trainingset/#3-creating-a-set-of-calibration-parameters","title":"3. Creating a Set of Calibration Parameters","text":"<p>One of the most important steps in creating good 3D ground truth annotations is precise camera calibration. As always we have a comprehensive section on how to record calibration recordings in our Manual. For this example we provide a set of example calibration recordings that you can download by clicking here. Go back to the homescreen of the AnnotationTool and select the Create new Calibration menu item. Like before let's first go through the options in the General section.</p> <ul> <li>Calibration Set Name is the name of the set of calibration parameters you're going to create.</li> <li>Calibration Set Savepath is the directory in which your new calibration parameters will be saved.</li> <li>Separate Recordings for Intrinsics can be set to no if you want to use your extrinsic recordings for intrinsics calibration. This is not recommended, see the manual for details.</li> <li>Intrinsics Folder Path is the directory that contains all the recordings for intrinsic calibration (you will find it inside the downloaded 'Calibration_Example' directory).  </li> <li>Extrinsics Folder Path is the directory that contains all the recordings for extrinsics calibration (you will find it inside the downloaded 'Calibration_Example' directory).</li> </ul> <p>Calibrating on your own data might require you to change some settings in the Calibration Settings and the Checkerboard Layout sections as well. For this example the defaults are already correct though, so you can leave all those settings untouched.</p> <p>Once you have entered all those settings you can click the Update Cameras button to add all cameras and pairs to the lists. After that you're almost ready to calibrate the cameras. The last thing you need to do is to delete the camera pair labeled 'Camera_LC --&gt; Camera_B' and instead add the 3 camera triplet 'Camera_T --&gt; Camera_LC --&gt; Camera_B'. The GIF below shows you how to do that in detail.</p> <p></p> <p>You're now ready to click the Calibrate Button. This will take a little bit, but once it is done a info window will pop up showing you the quality of all your calibrations. It should look similar to this:</p> <p></p> <p>You will now find a directory containing one '.yaml' calibration file per camera at the path that you specified.</p>"},{"location":"getting_started/3_creating_trainingset/#4-annotating-a-frameset","title":"4. Annotating a Frameset","text":"<p>Now that you have both a dataset and the calibrations that go along with it you can start annotating your framesets. First navigate back to the homescreen and select the Annotate Dataset item. Navigate to the directory of the dataset you created earlier and select the '.yaml' config file as shown below. You will then get an overview over the different segments of your dataset (in the example case there is only one) and a list of all your cameras. Select the segment you want to annotate and click the Load Dataset button.</p> <p></p> <p>With the dataset loaded succesfully all that's left to do is to add the calibrations you created earlier to the dataset. To do this click on the big blue plus sign on the left side of the screen and select the directory that contains the '.yaml' calibration files. After that you can start annotating. The workflow is as following: 1. Annotate all the frames in one frameset. You can switch between them either by clicking the Next &gt;&gt; and &lt;&lt; Previous on the left or by double clicking one of the cameras in the list. As soon as you annotate a keypoint in two or more cameras you will see a error bar appear in the Reprojection Tool. This is an indicator for the consistency of your annotations, the lower the better. 2. Switch to the next frameset once all reprojection error bars are sufficiently low. Once you annotated all joints for one frameset you can switch to the next one using the Next Set &gt;&gt; button. If you have a dataset consisting of more than one segment the dropdown in the top left corner will allow you to switch between segments.</p> <p></p> <p>For a real dataset it is important that you annotate all framesets in a dataset before proceeding to the trainingset exporting step. Since this is only a tutorial we suggest you play around with the tool long enough to get familiar with and and get a feeling of how the Reprojection Tool works. Once you feel comfortable you can move on to the next step.</p>"},{"location":"getting_started/3_creating_trainingset/#5-exporting-a-trainingset","title":"5. Exporting a Trainingset","text":"<p>Almost done! As always navigate back to the homescreen and select the last item in the list: Export Trainingset. As before there are a handful of parameters you need to set:</p> <ul> <li>Trainingset Name is the name of the trainingset you will create.</li> <li>Trainingset Savepath is the directory the trainingset will be saved in.</li> <li>TrainingSet Type lets you select if you want to create a 2D or a 3D trainingset. 3D trainingsets include the calibration parameters and are what you almost always will be using. Only use the 2D option if you are working with single camera data or you are creating a pretraining trainingset.</li> </ul> <p>The rest of the parameters are related to how the data is split into training and validation data. The defaults should be fine for almost all applications, so just leave them untouched for the example. After setting your parameters you can click the blue 'plus' button to add one or more dataset to the trainingset. For the example add only the dataset you did annotate earlier. Selection works just like in the annotation mode by selecting the '.yaml' dataset config file. After adding the dataset the pie chart in the bottom left corner should be show some statistics about your dataset. Since we did not annotate many frames the majority of keypoints will be unnanotated. If everything looks like shown below you can click the Create Trainingset button.</p> <p></p> <p>You should now have a trainingset that has the same structure as the one you used to train your first network.</p> <p> That's it! Now it's time to get started with training a model on your own data. If you want to learn more about our toolbox we strongly suggest you have a look at our Manual. There you will find detailed instructions on every step of building a 3D motion capture setup with JARVIS.</p>"},{"location":"manual/1_introduction/","title":"JARVIS Manual","text":"<p>This Manual covers all the basic steps you need to take to get from the idea of using markerless tracking to 3D pose predictions ready for analysis. This obviously makes it quite a lengthy read, so feel free to skip any sections that might not be relevant to you. The blue Troubleshooting boxes can be expanded by clicking on them and will contain some hopefully helpful hints if you encounter any issues.</p>"},{"location":"manual/2_installation_guide/","title":"Installation Guide","text":"<p>This covers the entire installation process for setting a Ubuntu 22.04 PC with all JARVIS Modules. Some of this might be mentioned in other parts of the Manual or the Getting Started Guide. You do not necessarily need to use Ubuntu 22.04 as your OS of course, but everything is developed and tested in Ubuntu, so chances of everything working out of the box are the highest this way!</p>"},{"location":"manual/2_installation_guide/#1-setting-up-linux","title":"1. Setting up Linux","text":"<ul> <li>Install Ubuntu 22.04 (you can use This Guide)</li> <li> <p>Install the Lambda Stack by openning a Terminal and running:     <pre><code>wget -nv -O- https://lambdalabs.com/install-lambda-stack.sh | sh - sudo reboot\n</code></pre>     This installs all Nvidia Drivers and other important dependencies. To test the install run this after the reboot:      <pre><code>nvidia-smi \n</code></pre>     The output should look like this:</p> <p></p> <p>If there is an error like <code>Failed to comminicate with NVIDIA driver</code>, disable secure boot in the BIOS.</p> </li> <li> <p>Install Miniconda by downloading the installer from here (Use <code>Miniconda3 Linux 64-bit</code>) and runnning     <pre><code>sh Miniconda3-latest-Linux-x86_64.sh\n</code></pre>     (If your installer is in your <code>Downloads</code> directory, make sure you also open your terminal in this directory)</p> </li> <li>Download FLIR Spinnaker (and Spinview) from their website (Use the Linux AMD 64-bit version). Complete the entire install, changing the udev-rules and USBFS size are also really important for Jarvis to work correctly. To install extract the downloaded archive to your home directory and from inside it run    <pre><code>sh install_spinnaker.sh\n</code></pre> During the install you can always select yes if prompted. Restart the computer after the installation is complete for all the changes to take effect.</li> </ul>"},{"location":"manual/2_installation_guide/#3-setting-up-the-acquisitiontool","title":"3. Setting up the AcquisitionTool","text":""},{"location":"manual/2_installation_guide/#installation","title":"Installation","text":"<ul> <li>Go to the Github Repos Releases Page and download the installer for your system from the most recent release</li> <li>Open a Terminal in the directory you downloaded the installer to and run    <pre><code>sudo apt install ./JARVIS-AcquisitionTool_&lt;VERSION&gt;.deb \n</code></pre>    Replace version with the one you downloaded of course!</li> <li>You should now be able to launch the AcquisitionTool by running <code>AcquisitionTool</code> in a terminal from anywhere</li> </ul>"},{"location":"manual/2_installation_guide/#setting-up-usb-cameras","title":"Setting up USB Cameras","text":"<p>Check that that USBFS size is set correctly by running <pre><code>cat /sys/module/usbcore/parameters/usbfs_memory_mb\n</code></pre> This should return <code>1000</code>. There is no further setup required for USB Cameras</p>"},{"location":"manual/2_installation_guide/#setting-up-gige-cameras-and-network","title":"Setting up GigE Cameras and Network","text":""},{"location":"manual/2_installation_guide/#network-switch","title":"Network Switch","text":"<p>The exact way to configure your network switch depends on the barnd and model. </p> <p>Set <code>MTU</code> to ~9014</p>"},{"location":"manual/2_installation_guide/#network-setup-on-computer","title":"Network Setup on Computer","text":"<ul> <li>Assign a fixed IP to all Ports on your Network Interface Card (NIC): <code>Settings -&gt; Network -&gt; Ethernet -&gt; Gear Icon -&gt; IPv4</code>:</li> <li>Set IPv4 Method to <code>Manual</code></li> <li>Set Address to: <code>192.168.1.1</code> (192.168.1.2 for the second port etc.)</li> <li>Set Netmask to <code>255.255.255.0</code></li> <li>Setup Maximum Transfer Unit (<code>MTU</code>) for all network ports:</li> <li>Go to <code>system-connections</code> directory using:       <pre><code>cd /etc/NetworkManager/system-connections/\n</code></pre></li> <li>List all files in the directory using <code>ls</code>, there should be one file per port with names like <code>Wired connection 2.nmconnection</code></li> <li>Edit each of those files by running (replace X with actual number):       <pre><code>sudo gedit Wired\\ connection\\ X.nmconnection\n</code></pre></li> <li>Below <code>[ethernet]</code> add the following line:       <pre><code>mtu=9014\n</code></pre></li> <li>After this reboot the computer and run this to check if it is configured correctly:       <pre><code>ifconfig | grep mtu\n</code></pre></li> </ul>"},{"location":"manual/2_installation_guide/#network-setup-on-cameras","title":"Network Setup on Cameras","text":"<p>For the first steps it is essential that the cameras are connected to a network with access to a DHCP server</p> <ul> <li>Open the Spinview Software (This step can **not be done with JARVIS) <li>For each camera that has a red exclamation mark, right click on it and assign a new IP using <code>Auto Force IP</code></li> <li>Now the camera should be accessible from SpinView, double click the first one to select it</li> <li>For each camera:</li> <li>Search for DHCP -&gt; uncheck checkmark</li> <li>Search for Current IP configuration persistent IP -&gt; set checkmark</li> <li>Search for Persistent IP Address -&gt; set to available address, e.g <code>192.168.1.2</code> (make sure this matches the IP of your network ports)</li> <li>Search for Persistent Subnet Mask -&gt; <code>255.255.255.0</code></li> <li> <p>IP addresses need to be entered as plain integers, conversion works as follows for <code>192.168.1.2</code> <pre><code>ip_int = (196*256^3) + (168*256^2) + (1*256^1) + (2*256^0) \n</code></pre></p> </li> <li> <p>Important: Connect the camera to their final Switch/Network now. For each Camera:</p> </li> <li>Search for Packet and check that <code>Max. Packet Size</code> is 9000 and set <code>SCPS Packet Size</code> to 9000. If <code>Max. Packet Size</code> is smaller there is an issue with your network configuration, go back to the two previous steps and make sure everything is set up correctly.</li>"},{"location":"manual/2_installation_guide/#setting-up-the-arduino-trigger","title":"Setting up the Arduino Trigger","text":"<ul> <li>Clone the JARVIS-TriggerFirmware Repository using:    <pre><code>git clone --recursive https://github.com/JARVIS-MoCap/JARVIS-TriggerFirmware.git\n</code></pre></li> <li>Setup the udev rules to be able to connect to the Arduino by running:    <pre><code>curl -fsSL https://raw.githubusercontent.com/platformio/platformio-core/develop/platformio/assets/system/99-platformio-udev.rules | sudo tee /etc/udev/rules.d/99-platformio-udev.rules\n</code></pre></li> <li>Navigate to the cloned Repository:   <pre><code>cd JARVIS-TriggerFirmware\n</code></pre></li> <li>Make sure your Arduino Uno is connected to one of the USB ports on your computer (a USB 2.0 port is sufficient here) and run    <pre><code>sh install_arduino_uno.sh\n</code></pre></li> </ul>"},{"location":"manual/2_installation_guide/#4-setting-up-the-annotationtool","title":"4. Setting up the AnnotationTool","text":"<ul> <li>Go to the Github Repos Releases Page and download the installer for your system from the most recent release</li> <li>Open a Terminal in the directory you downloaded the installer to and run    <pre><code>sudo apt install ./JARVIS-AnnotationTool_&lt;VERSION&gt;.deb \n</code></pre>    Replace version with the one you downloaded of course!</li> <li>You should now be able to launch the AcquisitionTool by running <code>AnnotationTool</code> in a terminal from anywhere</li> </ul>"},{"location":"manual/2_installation_guide/#5-setting-up-the-hyrbridnet-python-library","title":"5. Setting up the HyrbridNet Python Library","text":"<ul> <li>Install git using:   <pre><code>sudo apt install git\n</code></pre></li> <li>Clone the HybridNet GitHub Repository by running:    <pre><code>git clone https://github.com/JARVIS-MoCap/JARVIS-HybridNet.git\n</code></pre></li> <li>Navigate to the downloaded Repo with:   <pre><code>cd JARVIS-HybridNet\n</code></pre></li> <li>Create a <code>jarvis</code> Conda Environment by running:   <pre><code>conda create -n jarvis python=3.9  pytorch=1.10.1 torchvision cudatoolkit=11.3 notebook  -c pytorch\n</code></pre></li> <li>Activate the environment by running:   <pre><code>conda activate jarvis\n</code></pre></li> <li>Make sure the right <code>SetupTools</code> version is installed with:   <pre><code>pip install -U setuptools==59.5.0\n</code></pre></li> <li>Install the <code>jarvis</code> package with:   <pre><code>pip install -e .\n</code></pre></li> <li> <p>You should now be able to run the jarvis Command-Line-Inerface (CLI) with:   <pre><code>jarvis launch-cli\n</code></pre>   You should see this in your terminal:   </p> </li> <li> <p>To update Jarvis run   <pre><code>git pull\n</code></pre>   from inside the <code>JARVIS-HybridNet</code> directory.</p> </li> </ul>"},{"location":"manual/3_mocap_setup_design/","title":"Designing a 3D Motion Capture Setup","text":"<p>Designing a good Camera Setup for Motion Capture is probably the most critical step to achieve reliable tracking and will safe you a lot of frustration while annotating, training your network and analyzing your data. That being said, without a proper starting point the vast number of available cameras, lenses, lighting options and possible setup configurations can be quite overwhelming. That's why your first step should be to answer the following questions for your planned setup. The first set of questions will cover all the camera and lens specific decisions you'll have to make. Keep in mind that if you want to use our Acquisition Tool without any modifications you should use a camera model from our list of supported cameras.  </p> <ul> <li> <p>What is the size of the area my subject will move in and at what distance do I want to mount my cameras?  This will determine the field of view and with that the focal length of your lenses. You can use this handy Calculator to help you pick the correct values. If your setup requires lenses with a very short focal length, make sure to get lenses with as little distortion as possible.</p> <p></p> </li> <li> <p>What is the size of the smallest features you want to resolve?   This will determine the resolution of your camera sensor. Make sure you are able to annotate all the keypoints you want without to much difficulty at the resolution you choose.</p> <p></p> </li> <li> <p>What is the fastest speed my subject will move at?   This will determine both the minimum frame rate you need to record at and the maximum exposure time you can use. As a rough rule of thumb a framerate of 50 Hz works well for everyting that moves at human speed or lower, for fast moving subjects like monkeys or mice you'll want to go up to at least 100 Hz. If you want a more rigorous rule the Nyquist Theorem is your friend.</p> <p></p> </li> <li> <p>How long will the cables going from my cameras to the recording computer be?   This will determine whether GigE or USB3.0 cameras are the better choice for you. We generally recommend using USB cameras, but for distances longer than a couple of meters the ethernet based GigE cameras are the better choice.</p> </li> </ul> <p></p> <p>Now that the configuration of the individual cameras is out of the way, we can move on to the most important question regarding the whole setup.</p> <p>How precise does the tracking need to be for my application and how much occlusion (both by other objects in the setup and by the subject     itself) do I expect?</p> <p>This is the most important question as it determines the number of cameras you will need to use. First of all think about how many cameras fit into your budget keeping in mind that more cameras also require a more powerful recording computer. You can then determine the absolute minimum number of cameras that you need by making sure that every keypoint you want to track is visible in at least two cameras at all times, as illustrated in the sketch below. Note that this is the absolute minimum and depending on the precision you require we recommend to have every keypoint visible in at least three or four cameras at all times.</p> <p></p> <p>While thinking about your camera configuration try to make the angles between the cameras as wide as possible. Ideally you want to distribute your cameras as evenly as possible on a sphere around your tracking volume. The sketch below again tries to illustrate that principle.</p> <p></p> <p>With that all of the basic design decisions should be covered and the only thing left is a list of some of the easily overlooked but still very important things to consider:</p> <ul> <li>There are two ways to go about mounting your cameras. You can either build a very rigid and permanent mounting system, or one that is flexible and easily repositioned. Both have their obvious advantages and disadvantages and you need to decide what fits your setup structure best. But keep in mind that by going for a flexible system you will have to recalibrate your setup EVERY time you use it. Accurate calibrations are the foundation of precise 3D tracking and its hard to stress its importance enough.</li> <li>Make sure you design your setup in a way that allows you to record good calibration videos. You can check out our example calibration recording to get an idea of how that looks like.  </li> </ul> <p>To close of this section, here is a render of the basic structure of our 12 camera hand-tracking setup, incase you need some inspiration:</p> <p></p>"},{"location":"manual/4_acquisitiontool_setup/","title":"Setting up the AcquisitionTool","text":"<p>With the basic setup design out of the way, the next challenge to tackle is getting all your cameras to record synchronized videos for you. Our AcquisitionTool makes this process very easy for all FLIR machine vision cameras.</p>"},{"location":"manual/4_acquisitiontool_setup/#1-hardware-requirements","title":"1. Hardware Requirements","text":"<ul> <li>A set of FLIR cameras: We recommend the BlackFly S model, but choose whatever fits your application best.</li> <li>Matching GPIO cables: Those are required to hook up the external trigger, you can order them with your cameras on the FLIR website.</li> <li>An Arduino Uno (or similar): This will be used as the source of the external trigger (And as a way to control and monitor your setup in future releases).</li> <li>A recording computer with the following specs:</li> <li> <p>A proper lighting solution: This one really depends on your setup, so we can't give any exact recommendations. But make sure you don't forget to take proper lighting into account when designing your setup. The most important factors here are even illumination from all sides and overall brightness.</p> <ul> <li>A recent Nvidia GPU: 10xx, 20xx and 30xx series cards will work. If you are using more eight cameras or more we recommend at least a 2080 or preferably a 30-series card.</li> <li>A decent CPU Our tool offloads most of the work to the GPU, a modern Intel i7 or equivalent is strongly recommended.</li> <li>A fast SSD Even with compression you are still writing a lot of data. We recommend SSDs with a write speed of at least 3000 MB/s.</li> <li>Enough USB Ports: Make sure you have enough available USB3 ports if you are using USB cameras. FLIR sells some suitable USB Host Controller Cards. </li> </ul> <p>Be aware of USB bandwidth sharing when using motherboard usb ports</p> <p>Caution: Some USB ports on your computer can share their bandwidth, which will cause issues if you are recording at high resolutions. You can check that in the spec sheet of your motherboard or the Hardware check tab in the AcquisitionTool Settings.</p> </li> </ul>"},{"location":"manual/4_acquisitiontool_setup/#2-software-installation","title":"2. Software Installation","text":"<p>The first thing you will have to do is install the FLIR Spinnaker SDK. You can download it here. If you are running Windows make sure to download the '*_x64.exe' found in the 'Latest Spinnaker Full SDK' directory. For Linux the '*amd64-pkg.tar.gz' is the package you want. Once that is installed you can grab the AcquisitionTool Installer from the Downloads section. Under Windows just run the installer and follow the instructions. Under Linux you can install the AcquisitionTool by running <pre><code>sudo apt install ./JARVIS-AcquisitionTool_1.0-1_amd64_2004.deb\n</code></pre> (Make sure to replace the version numbers with the version you downloaded).</p> <p>If the installation completed successfully the AcquisitionTool should now be available in your Start menu under Windows. If you are running Linux you can open it by typing <code>AcquisitionTool</code> into a terminal and pressing enter.</p> <p>This is a good time to test out if everything is working as intended, before we move on to setting up synchronization with the external trigger. For a quick test connect at least one or two cameras to your computer, launch the AcquisitionTool and navigate to the Connection mode as shown below. You can now either connect each camera individually by clicking one of the  slots or simply detect all cameras with the Auto Detect Cameras button. If that works you can then switch back to the Acquisition Mode. If your cameras are in their default mode you should be able to get them streaming by clicking the   button in the top left corner. If you now see live images of what all your cameras are seeing you are set to move on to the next and final setup step for the AcquisitionTool.</p> <p></p> Troubleshooting and Hints <ol> <li> <p>Cameras not connecting: If your cameras do not show up after clicking the Auto Detect button there's a couple of things you can do:</p> <ul> <li>Make sure your camera is plugged into a (not shared) USB3 port (Usually the ones with the blue plastic part).</li> <li>Check whether you are able to access them in SpinView. If that also fails try reinstalling the Spinnaker drivers.</li> <li>Linux only: Make sure you did setup the group permissions and the USB buffer size correctly.</li> </ul> </li> <li> <p>Cameras not streaming: If any of your cameras are not showing an image after you start streaming this is very likely due to them being set up in a non standard acquisition mode. To make sure they have their default settings loaded you can follow the these steps:</p> <ul> <li>Select a camera by double clicking its name in the list in the top left corner</li> <li>Click the   arrow in the top right corner of the 'Camera Settings' Tab. This should open up the presets menu.</li> <li>In the menu select the 'Default' UserSet by clicking on it and press 'Load'.</li> <li>Try streaming again by clicking the green  button, it should now work for the camera you selected.</li> <li>Repeat those steps for all cameras that are not showing an image.</li> </ul> </li> <li> <p>Building the Tool from source: If you are planning on using the AcquisitionTool on a different Linux distribution or are interested in modifying the source code check out the GitHub Repo! It has detailed instructions on how to build the tool and its dependencies yourself.</p> </li> </ol>"},{"location":"manual/4_acquisitiontool_setup/#3-setting-up-the-external-trigger","title":"3. Setting up the External Trigger","text":"<p>At this point you should have a recording setup that can be controlled using our AcquisitonTool and can stream video from all of your cameras. The last but very important step that is still missing is making sure all cameras record their videos perfectly in sync. To do this we use an external trigger pulse supplied by an Arduino Uno (or similar, our PlatformIO project supports many of the commonly used Microcontrollers).</p>"},{"location":"manual/4_acquisitiontool_setup/#programming-the-arduino","title":"Programming the Arduino","text":"<p>Programming the Arduino is really easy thanks to our PlatformIO install scripts for both Linux and Windows. Simply do the following:</p> LinuxWindows <ul> <li> <p>Clone our TriggerFirmware Repository from GitHub with: <pre><code>git clone --recursive https://github.com/JARVIS-MoCap/JARVIS-TriggerFirmware.git\n</code></pre></p> </li> <li> <p>Change into the TriggerFirmware directory and execute the install script: <pre><code>cd JARVIS-TriggerFirmware &amp;&amp; sh install_arduino_uno.sh\n</code></pre></p> </li> </ul> <ul> <li>Make sure you have a recent version of Python installed. You can either get it directly from the Microsoft Store or download it from here.</li> <li>Clone our TriggerFirmware Repository from GitHub with: <pre><code>git clone --recursive https://github.com/JARVIS-MoCap/JARVIS-TriggerFirmware.git\n</code></pre></li> </ul> <p>Do not use the <code>Download ZIP</code> option on GitHub</p> <p>GitHub does not include all neccessary submodules in its .zip download. If you don't have git installed on your computer you can click here to download a .zip folder containing all the necessary data.</p> <ul> <li>Change into the TriggerFirmware directory and execute the installer batch file: <pre><code>.\\install_arduino_uno.bat\n</code></pre></li> <li>If the install throws an error related to <code>Long Path Support</code> first remove the <code>JARVIS-TriggerFirmware\\PlatformIO\\install</code> directory and then open a command prompt as administrator and run: <pre><code>reg add \"HKLM\\SYSTEM\\CurrentControlSet\\Control\\FileSystem\" /v LongPathsEnabled /t REG_DWORD /d 1\n</code></pre></li> </ul>"},{"location":"manual/4_acquisitiontool_setup/#wiring-up-the-arduino","title":"Wiring up the Arduino","text":"<p>Now comes the slightly tricky part. wiring up all the trigger cables. The exact connections you have to make depend on your exact camera model, but the general idea is always the same:</p> <ul> <li>Connect the ground (GND) pins of all cameras to one of the pins on the Arduino labeled GND.  </li> <li>Connect the trigger input (check your cameras datasheet) pin of all cameras to pin 6 of your Arduino.</li> </ul> <p>For more details on how to hook up your specific camera this guide from FLIR might be helpful. Here is a very basic wiring diagram for the FLIR BlackFly S:</p> <p></p> <p>Once your Arduino trigger system is all wired up you can go back to the AcquisitionTool and connect the trigger using the  button. To make the cameras use the trigger signal there are a few settings you will have to change on each camera. Again, the exact settings might vary slightly depending on your camera model. This guide shows the settings for the BlackFly S, check FLIRs documentation on your camera model if those settings don't work. Here's the step-by-step guide:</p> <ol> <li>Select a camera by double clicking its name in the list in the top left corner</li> <li> <p>Make sure the cameras default settings are loaded:</p> <ul> <li>Click the  arrow in the top right corner of the 'Camera Settings' Tab. This should open up the presets menu.</li> <li>In the menu select the 'Default' UserSet by clicking on it and press 'Load'.</li> </ul> </li> <li> <p>Click the  button to get access to all settings.</p> </li> <li> <p>Here's all the settings you will have to change:</p> <ul> <li>Trigger Mode: On</li> <li>Trigger Source: Line3</li> <li>Trigger Overlap: ReadOut</li> <li>Pixel Format: BayerRG8</li> <li>Exposure Auto: Off</li> <li>Exposure Time: Needs to be shorter than the time between your frames (At 100 FPS the limit is 10 ms). Caution: The value is set in microseconds!</li> </ul> </li> </ol> <p>With all those settings adjusted you should now be able to record synchronized videos! To be sure it's best to do a test recording and checking if all videos have exactly the same length.</p>"},{"location":"manual/4_acquisitiontool_setup/#4acquisitiontool-overview","title":"4.AcquisitionTool Overview","text":"<p>The following will describe how to test and configure a multi-camera setup using the AcquisitionTool, these steps are identical for USB and GigE cameras. Here's a quick overview of main the interface:</p> <p></p> <ul> <li>The camera list on the upper left corner lists all connected cameras and lets you toggle between them</li> <li> <p>The control panel on the lower left side gives you acces to each cameras settings. You can select the active camera by double clicking the camera in the list above.</p> <p>Settings are applied only to the active camera, the only way to change settings on all cameras simultanously is the Setup all Cameras button.</p> </li> <li> <p>The button with the  icon lets you switch between the basic settings and all available settings.</p> </li> <li>Saving and loading Presets to the cameras is done using the  (for saving) and  (for loading) buttons. Only UserSet0 and UserSet1 can be overwritten, Default loads the factory settings of the camera. </li> <li>The control panel has two more tabs:</li> <li>Trigger Settings: This tab contains the settings for the external trigger. The only one that's relevant for most usecases is the FrameRate (FPS), which sets the frame rate in Frames Per Second.</li> <li>Performance: Shows the buffer useage and latency of each camera. The buffer useage bar should always be green and ideally not moving. The latency (enabled by clicking the  button) should be lower than the intervall between captured frames. (Example: at 100 FPS the intervall between frames is 1000ms/100 = 10ms).</li> </ul>"},{"location":"manual/4_acquisitiontool_setup/#1-connect-cameras-and-save-setup-configuration","title":"1. Connect Cameras and save Setup Configuration","text":"<ul> <li>Launch the AcquisitionTool by running <code>AcquisitionTool</code> in a terminal</li> <li>Make sure your cameras are connected to your PC, the indicator LEDs on the cameras should be flashing in green at this point</li> <li>Switch to the connection Tab and click the Auto Detect Cameras  button.     </li> <li>You can change the name of each camera by double clicking the name field (the defaults are Camera_0 to Camera_N). Important: Do not use the \"-\" character in the camera names!</li> <li>Click the  button next to the Auto Detect Cameras  button to save the camera configuration (this includes which cameras are connected and their names) as a Preset. </li> <li>Presets can be loaded using the <code>blue Arrow</code></li> <li>Make sure your Arduino with the <code>TriggerFirmware</code> installed (see this Section) is connected to a USB port on the computer and click the <code>green Plus</code> on the right to attach it to the AcquisitionTool. It will usually show up with a Device Name of <code>0043</code>, but it might be different.</li> <li>Important: Make sure that all connected devices show <code>Ready...</code> as their status! It is expected for the Trigger to show <code>Error...</code> for a few seconds when connecting.</li> </ul>"},{"location":"manual/4_acquisitiontool_setup/#2-configuring-the-cameras-for-use-with-the-external-trigger","title":"2. Configuring the Cameras for use with the external Trigger","text":"<ul> <li>Switch back to the <code>Camera</code> tab in the toolbar on the left edge of the window. You should see all your cameras in the list in the upper left corner.</li> <li>Click the <code>Setup All Cameras</code> button in the control panel. Leave both options under <code>General</code> checked and set the <code>Exposure Time</code> and <code>Gain</code> to suitable values. Important: Automatic Exposure is not recommended, as long exposure times can cause issues with the external Trigger.</li> <li>Verify that all cameras are setup correctly, the key settings to check are the following:</li> <li><code>Trigger Mode -&gt; On</code></li> <li><code>Trigger Source -&gt; Line3</code></li> <li><code>Trigger Overlap -&gt; Readout</code></li> <li><code>Pixel Fromat -&gt; BayerRG8</code></li> <li>After verifying the settings of each camera you can save them as a Preset (e.g. UserSet0), using the <code>green Arrow</code> button.</li> <li>Set <code>UserSet0</code> as the UserSet to be loaded on powerup using the <code>blue Arrow</code> button</li> <li>Try out your configuration by clicking the <code>green Play</code> button to start streaming from your cameras. This will not save any videos! Check that all cameras are working and no errors appear in the camera list. To stop streaming click the <code>red Stop</code> button. Make sure to select an appropriate Frame Rate for your cameras before streaming or recording!</li> <li>Important: If you experience any issues at this stage here are some things to check: </li> <li>For GigE cameras only: Confirm that the <code>Max. Packet Size</code> is set to <code>9000</code> and that your Switch as well as Network Interface Card have sufficient bandwidth. (E.g. not more than 10 cameras on a single 10 Gigabit connection).</li> <li> <p>For USB3 cameras only: Make sure the USB Host Controller Card and/or the Motherboard USB ports you are using have sufficient bandwidth. You can check this by opening the <code>Settings</code> in the top right corner of the window, switching to the <code>Hardware Check</code> tab and running 'Check System':</p> <p></p> </li> <li> <p>For USB3 cameras only: Check that your USB-FS buffer size is set correctly by running:     <pre><code>cat /sys/module/usbcore/parameters/usbfs_memory_mb\n</code></pre>     This should return <code>1000</code>. If not you can temporarily increase it by running:     <pre><code>echo 1000 &gt; /sys/module/usbcore/parameters/usbfs_memory_mb\n</code></pre>     If this fixes it run the <code>configure_usbfs.sh</code> that is located in your FLIR Spinnaker install directory.</p> </li> </ul>"},{"location":"manual/4_acquisitiontool_setup/#3-record-some-videos","title":"3. Record some Videos","text":"<p>To test the video recording functionality of the system follow these steps: - Make sure your external trigger is connected and everything is set up as described in the previous section - Click the <code>blue Folder</code> icon in the top bar and select a directory to store your recording in. Important: Make sure that the directory is on a SSD with fast write speeds (&gt; 2500MB/s). - Give the recording a unique name, if you leave this field blank the name will be \"Recording_{Timestamp}\". - Click the <code>red Circle</code> recording button to start the recording. You should see a live preview of all camera perspectives. Leave this running for a short amount of time and stop the recoding using the <code>red Stop</code> button. - Navigate to the directory you selected earlier and check that all cameras have recorded videos and you can play them back. - Open the <code>metadata.csv</code> file and check that the <code>frame_id</code> field contains the same number (+- 2 frames is okay here, this does not mean that the videos are out of sync, but there is a slight issue with the metadata file writer getting closed a little too early) - To compress the videos further after recording, you can use the following FFMPEG command for each cameras video file:   <pre><code>ffmpeg -i Camera_T.avi -c:v libx265 -crf 12 -preset superfast -tag:v hvc1 Camera_T.mp4\n</code></pre>   Obviously replace \"Camera_T.*\" with your actualy camera name. </p>"},{"location":"manual/5_recording_calibration/","title":"Recording Good Calibration Videos","text":""},{"location":"manual/5_recording_calibration/#intrinsics-calibration","title":"Intrinsics Calibration","text":"<p>The first step to get your camera calibration files is to record one calibration video for each camera. This video will be used to compute its focal length, principal point offset and distortion parameters. Or in simpler terms: all the camera specific parameters that we need for 3D reconstruction.</p> <p></p> <p>There are a few rules you have to follow to get the best intrinsics calibration possible:</p> <ul> <li>Move the checkerboard along all axis (especially rotation!)   Make sure you do not only record frames with the checkerboard parallel to the camera (as shown in the picture on the left). Not rotating it enough makes it impossible for the calibration tool   to estimate the focal length correctly.</li> </ul> <p></p> <ul> <li>Fill as much of the field of view with the checkerboard as possible   Make sure to be close enough to the camera to take advantage of the full resolution of your camera to cover at least 2/3   of the cameras field of view. Obvioulsy it's just as important to not get so close the camera looses focus, a bigger checkerboard will help in those cases.</li> </ul> <p></p>"},{"location":"manual/5_recording_calibration/#extrinsics-calibration","title":"Extrinsics Calibration","text":"<p>The second step is to select a primary camera. This camera defines your reference frame when doing 3D reconstruction. To calibrate the extrinsic parameters you will have to record videos for all possible camera pairs that contain the primary camera. Those videos will be used to calculate the position of all secondary cameras relative to the primary camera.</p> <p></p> <p>The main thing to watch out for during extrinsics recordings is that both cameras have a good and unoccluded view of the checkerboard. As long as that's the case it is as simple as just waving the board around as much as you can.\\ Side note: While it is strongly recommended to record seperate videos for intrinsics and extrinsics it is possibly to use your extrinsic recordings for intrinsics calibration. If you do so make sure to still follow the intrinsic calibration rules during your recordings.</p> <p>The main thing to watch out for during extrinsics recordings is that both cameras have a good and unoccluded view of the checkerboard. As long as that's the case it is as simple as just waving the board around as much as you can.\\</p>"},{"location":"manual/6_creating_and_labeling_datasts/","title":"Creating and Labeling Datasets","text":"<p>The AnnotationTool lets you create Datasets from your recordings, calibrate your cameras, annotate your Datasets, and finally export Trainingsets for training the HybridNet!</p> <p></p> <p>The following will show you how to use all parts of the AnnotationTool. There are four main functions:</p> <ul> <li>Create new Dataset: This lets you create Datasets from AcquisitionTool Recordings. This is also where you will define the Keypoints and Skeleton you want to annotate.</li> <li>Create new Calibration: This lets you create a set of calibration parameters from a set of checkerboard recordings</li> <li>Annotate Dataset: This lets you load and annotate a dataset</li> <li>Export Trainingset: Once you are finished annotating a dataset this lets you export it as a Trainingset, the format used by the HybridNet Pytorch module.</li> </ul>"},{"location":"manual/6_creating_and_labeling_datasts/#create-new-dataset","title":"Create new Dataset","text":"<ul> <li>(1): Configuration for the Dataset to be created:</li> <li>New Dataset Name: Name of the dataset to be created. A directory with the same name will be created</li> <li>New Dataset Path: Path where the new directory for the dataset will be created. This should not include the new datasets name!</li> <li>Framesets to extract per Segment: Number of FrameSets to be created per Segment (See (2) details about Segments). One FrameSet is a set of images from all cameras, this is not the number of images! The total number of FrameSets extracted per recording should not exceed 100 unless it is a very long recording.</li> <li>Sampling Method: Method used to extract the dataset. Select kmeans for high dataset quality and uniform for extraction speed. uniform is only recommended for quick test runs, not final datasets.</li> <li>(2): Module to add Recordings to the dataset and define Segments within them.</li> <li>The Add Recording Button lets you add a directory containing multi-camera recordings. The directory name will be used as the name of the recording, the filenames of the videos will be used as the camera names. Only add recordings with identical camera configurations and calibrations here!</li> <li>Click the Scissors button to open the Segmentation Module for a recording (see section below for more details)</li> <li>(3): Entities Input - This is a future-prrofing feature for potential multi-entity tracking capabilites. Simply add a single entity (e.g. \"Monkey\", or \"Hand\") here using the Add button.</li> <li>(4): Keypoint Input - Add the names of all the keypoints you want to label here using the Add button (e.g. \"Right Elbow\", \"Right Wrist\" etc.). You can change the order of the keypoints using the arrow buttons. The order will be used in all subsequent steps.</li> <li>(5): Skeleton Input - This step is optional! Add a skeleton connection and it's measured length (e.g. \"Right Lower Arm\", connecting the \"Right Elbow\" and \"Right Wrist\" keypoints). This will be used for visualization purposes as well as for showing the deviation between the measured lengths and the ones calculated for each FrameSet.</li> <li>(6): Save and load Presets using these buttons. This is highly recommended, to make sure no typos ruin the compatability between two datasets with the same Keypoint configuration. A Hand and Rodent Body preset are provided. You can also import a configuration from a already existing dataset. </li> </ul>"},{"location":"manual/6_creating_and_labeling_datasts/#segmentation-module","title":"Segmentation Module","text":"<p>The Segmenation Module allows you to define which parts of a recording will be used to create the dataset to be labeled. This is a critical step in cases where there are long phases of inactivity between the relevant activities (e.g. long resting phases between grasps in our monkey experiments). As mentioned above an equal number of FrameSets is extracted for each segment. </p> <p>Segments with the same name are treated as one disjunct segment!</p> <p></p> <ul> <li>(1): Editor Timeline - Scrub through the video using the green cursor, zoom in and out using your mouse wheel and drag the gray bar to get to different parts of the video. </li> <li>(2): Click this button to add a segment. The segment will show up in (3) and a blue <code>Range Cursor</code> will appear in the timeline. Drag it around to select start and endpoint of the video and click <code>Add</code> to finalize the segment.</li> <li>(3): Segments list - Doule click a segment to change it's name or delete it using the red X button. Segments with identical names will be treated as one disjunct segment.</li> <li>(4): Double click a camera to change the view displayed in the editor.</li> <li>(5): Save and load segmentations as <code>.csv</code> files. This is strongly recommended as redoing a segmenation can be a lot of work. Also save periodically to avoid dataloss in case of any crashes.</li> <li>Click continue once you are happy with your segmenation, your segmenation should be displayed next to your recording now.</li> </ul>"},{"location":"manual/6_creating_and_labeling_datasts/#create-new-calibration","title":"Create new Calibration","text":"<p>This module lets you create calibration parameters (in the form of <code>.yaml</code> files) from a set of calibration recordings. For more information on camera calibration in general, the OpenCV Documentation is a good start. Check out this Section for instructions on how to record calibration videos</p> <p></p> <ul> <li> <p>(1): Configuration:</p> <ul> <li>General:<ul> <li>Calibration Set Name: Select the name of your new set of calibration parameters. This will be the name of the directory they are stored in.</li> <li>Calibration Set Savepath: Path where the new directory will be created.</li> <li>Seperate Recodrings for Intrinsics: Select wether you have seperate recordings for the intrinsic parameter calibration. If you do the format should be the following: One directory containing one video per camera, the filename has to be identical to the name of the camera. If you select \"No\" the Extrinisics recordings will be used for intrinsics calibration, this is generally not recommended.</li> <li>Intrinsics Folder Path and Extrinsics Folder Path are the path to the the intrinsics and extrensics recordings mentioned above. See the Examples folder for example calibration recordings.</li> <li>Update Camera Names: Click this after configuring all parameters in the General section. This will automatically detect all you Cameras and available Camera Pairs. Important: This will include all possible Camera Pairs, make sure you modify them as described in section (3) below!</li> </ul> </li> <li>Calibration Settings:<ul> <li>Max. Number of Frames for Intrinsics/Extrinsics Calibration should generally be left at the default value</li> <li>'Save Debug Images' saves images of all detected Checkerboards in a Debug folder along with your calibration parameters. This is very usefull for debugging a bad calibration.</li> </ul> </li> <li>Checkerboard Layout: <ul> <li>Board Type: Select wether you are using a normal checkerboard (Standard) or a ChArUco board. Normal checkerboard calibration is more precise and is recommended.</li> <li>Pattern Width and Pattern Height let you define the size of your checkerboard. Important: Make sure your checkerboard matches the generated image exactly, the way the pattern size is defined is not intuitive.</li> <li>Side Length [mm] lets you set the size of a single checkerboard square. Make sure this is accurate, as this defines the overall scale of the 3D reconstruction.</li> </ul> </li> </ul> </li> <li> <p>(2): Cameras - This will give you a list of all cameras that will be calibratet. Make sure all cameras are present, the order does not matter.</p> </li> <li>(3): Camera Pairs - This lists all configured Camera Pairs for extrinsic calibration. Make sure that the first camera listed (the primary camera) is the same for all datasets. You can modify a Camera Pair by double clicking it.</li> <li>Click the Calibrate button after you are done configuring your calibration run. This will take a while. After it is done you should see an overview of your calibration run:</li> </ul> <p></p> <p>All <code>Reprojection Erros</code> should be below a pixel at the very most. Generally a lower value is better and they should be fairly equal across all cameras and pairs. - If the calibration does not succeed or the reprojection Error is too high, first ensure that the checkerboard parameters are setup correctly. Next check the debug images to see if there are any issues with lighting or motion blur. If none of this helps, redo the calibration videos.</p> <p>If you are using three-camera chains during calibration it is expected that the reprojection error for those pairs is slighly higher than all others.</p>"},{"location":"manual/6_creating_and_labeling_datasts/#annotate-dataset","title":"Annotate Dataset","text":"<ul> <li>(1): Dataset Control - This lets you select the segment to be annotated in the dropdown at the top. Double clicking a camera in the list below selects the cameras frame for annotation.</li> <li> <p>(2): This is the main editor, the controls are as follows:</p> <ul> <li>Right Mouse Button: Place the active keypoint</li> <li>Left Mouse Button: Drag and move an existing keypoint</li> <li>Middle Mouse Button: Delete an existing annotation</li> <li>Mouse Wheel: Zoom in and out</li> <li>Ctrl + Left Mouse button: Pan the image</li> <li>Arrow Key Right: Change to next camera view</li> <li>Arrow Key Left: Change to previous camera view</li> <li>Arrow Key Down: Change to next FrameSet</li> <li>Arrow Key Up: Change to previous set</li> </ul> </li> <li> <p>The active Keypoint is indicated by the green highlighting in (3), you can change it by clicking any of the keypoints in the list.</p> </li> <li>A transparent annotation marker indicates that a keypoint is reprojected (automtically placed) and a solid annotation marker indicates that a marker is placed manually.</li> <li>Similarly a purple checkmark in (3) indicates a reprojected keypoint, a blue checkmark indicates a manually annotated keypoint.</li> <li>(4): ReprojectionTool - this displays the reprojection error for each joint that is annotated in at least two different camera views. Make sure these values stay as low as possible, and don't exceed ~5 pixels. </li> <li>(5): Buttons to switch between the different camera views</li> <li>(6): Buttons to Crop and Pan the editor view, Home resets the view</li> <li>(7): Buttons to switch between FrameSets</li> <li>(8): Viewer Button - Opens the 3D Viewer Window that shows a 3D reconstruction of all annotated keypoints. If a skeleton was defined during dataset creation, this will also be rendered here. </li> </ul> <p></p> <p>On some computers this causes a memory leak, due to a bug in the underlying library, if you experience sporadic crashes try avoiding the 3D viewer.</p>"},{"location":"manual/6_creating_and_labeling_datasts/#export-trainingset","title":"Export TrainingSet","text":"<ul> <li> <p>(1): Configuration:</p> <ul> <li>TrainingSet Name: This is the name of the directory that will be created for the new Trainingset.</li> <li>Trainingset Savepath: Path where the new Trainingset directory will be created</li> <li>Trainingset Type: Set wether to include calibrations in your dataset (3D). This should in be left at 3D for almost all usecases.</li> <li>Validation Split Fraction: This sets the fraction of the data that is used as validation data during training. If this is set to 0.1, 90% of the annotated FrameSets will be used for training and 10% for validation.</li> <li>Shuffle before Train/Validation Split: Select wether or not your FrameSets should be shuffled before splitting them into train and val splits. This should always be enabled.</li> <li>Random Shuffle Seed: Enable this if you need a truly random shuffle. Generally should be set to No.</li> <li>Shuffle Seed: Selects the seed for the dataset shuffling. This allows you to recreate a train/val split after modifying some annotations.</li> </ul> </li> <li> <p>(2): Datasets: This let's you select multiple annotated Datasets to combine into one Trainingset. They can use different calibrations, but the number and names of cameras have to be identical across all combined datasets. Double clicking a dataset allows you to exclude individual segments.</p> </li> <li>(3): Entities: This can be ignored, as only single entity tracking is currently implemented.</li> <li>(4): Keypoints: Lets you exclude individual keypoints from the created Trainingset.</li> <li>(5): Summary: Shows an overview of the composition of your Trainingset. Hover over a segment in the leftmost piechart to get details about the composition of individual datasets.</li> <li>(6): Buttons to save and load your settings as a preset, this is helpfull if you continously modify datasets and want to update your trainingsets. </li> </ul>"},{"location":"manual/7_training_hybridnet/","title":"Training HybridNet","text":"<p>First make sure you have the HybridNet Pytorch Library installed as described in this Section. </p> <p>You should be able to activate the Anaconda environment by running <pre><code>conda activate jarvis\n</code></pre> and then launch the GUI web-interface with <pre><code>jarvis launch\n</code></pre> This should open the following window in your browser:</p> <p></p> <p>From here you have two options:</p> <ul> <li>Create a new Project by entering a project name and the path to a TrainingSet you have exported with the AnnotationTool (Sorry for the inconsistent naming scheme!). You can use the same path for Dataset3D and Dataset2D for all usual usecases. <ul> <li>To check that everything is created correctly switch to the Visualization Tab and then play around with Visualize Datasset 2D/3D to see that all the markers are in the expected locations and the 3D reconstruction looks reasonable. </li> <li>New projectes are saved in the projects drirectory in the root JARVIS-Hybridnet directory</li> </ul> </li> <li>Load an existing Project by selecting it in the dropdown on the right. An example Project is provided to familiarize yourself with the interface.</li> </ul> <p>After either creating or loading a project you should see the following:</p> <p></p>"},{"location":"manual/7_training_hybridnet/#training","title":"Training","text":"<p>The first step after creating your project is training the different neural networks. You have a couple of options here:</p> <ul> <li>Train All: This trains all parts of the HybridNet architecture in sequence. It is strongly recommended to use this function, with default parameters, to train your networks intially. If this works and you get reasonable results you can use the projects <code>config.yaml</code> file to fine tune the parameters if you want to.</li> <li>Train Center: Trains the CenterDetect network. This is the first step in the tracking pipeline and is used to detect the center of your tracking subject. </li> <li>Train Keypoint: This trains the 2D keypoint detection network. This can be used on its own to detect keypoints in individual images.</li> <li>Train HybridNet: This trains the 3D-CNN that is the final stage of the tracking pipeline.</li> </ul> <p>Use the three individual training options only if you know what you are doing.</p> <p>Training will take anywhere from a few minutes to a few hours, depending on your dataset size and hardware. The interface should display updating loss-curves that shoudl look similar to the image below.</p> <p> </p> Tensorboard logs are saved the <code>logs</code> directory for every training run, models are saved in the <code>models</code> directory."},{"location":"manual/7_training_hybridnet/#prediction","title":"Prediction","text":"<p>The next step is to use your trained networks to predict the poses in one of your recordings. Select the Prediction tab and fill in the following details:</p> <ul> <li> <p>Path of recording directory is the path of the recording you want to run prediction on.</p> </li> <li> <p>Weights for CenterDetect / HybridNet lets you specify which weights you want to use. If you have trained models yourself you can leave them at latest. If you didn't train the network yourself you will have to put the path of the pretrained weights here. They can be found in the pretrained directory inside your JARVIS-Hybridnet folder.</p> </li> <li> <p>Start Frame &amp; Number Frames lets you select which part of the recording you want to run the prediction. For quick results set 'Number of Frames' to 1000. To predict until the end of the recording set it to -1.</p> </li> </ul> <p>Once you're done you can run the prediction by clicking the Predict button.\\ Once the process is finished you will find a directory with a current timestamp in the projects folder under predictions. That folder contains a 'data3D.csv' file that contains the 3D coordinates and their corresponding confidences for every point in time. The directory also contains a .yaml file that holds some information necessary for creating videos from your predictions.</p> Dataset3D file format: <ul> <li>The unit of the entries in the <code>data3D.csv</code> is mm</li> <li>The reference coordinate frame is relative to your primary camera. </li> <li>The <code>confidence</code> column has a number between 0 and 1, where one presents max. confidence. This metric is pretty flawed, so use carfully.</li> </ul>"},{"location":"manual/7_training_hybridnet/#visualization","title":"Visualization","text":"<p>Navigate to the Visualization tab. Here the correct prediction directory should already be selected. If you want you can remove or add cameras from the list of cameras for which you want to create annotated videos. You can now click Create Video as shown below. If everything is set correctly you should find a directory containing your freshly labeled videos in the project directory after the progress bar is filled up.</p>"},{"location":"manual/7_training_hybridnet/#analysis","title":"Analysis","text":"<p>If you want to compare the quality of different training runs in more detail the functions inside the Analysis tab are very helpfull.</p>"},{"location":"manual/7_training_hybridnet/#setting-the-config-parameters","title":"Setting the config parameters","text":"<p>Jarvis tries to derive a reasonable set of config parameters by analyzing your trainingset. This is not always perfect though and you will often find yourself modifying the config.yaml inside your project directory. This section aims to describe and illustrate all the non self-explanatory parameters.</p> ExampleProject config.yaml<pre><code>#General Configuration\nDATALOADER_NUM_WORKERS: 4       #Number of threads used for dataloading\n\n#Dataset Configuration\nDATASET:\n  DATASET_2D: Example_Dataset   #2D dataset path (usually same as DATASET_3D)\n  DATASET_3D: Example_Dataset   #3D dataset path\n\n#EfficientTrack 2D Center Detector Configuration:\nCENTERDETECT:\n  MODEL_SIZE: 'small'           #Can be 'small', 'medium' or 'large'\n  BATCH_SIZE: 8                 #Set to 4 for very small datasets (&lt;500 Frames) \n  MAX_LEARNING_RATE: 0.01       #Max learning rate in OneCycle schedule\n  NUM_EPOCHS: 50                #Set to 100 for very small datasets\n  CHECKPOINT_SAVE_INTERVAL: 10  #Saves a .pth checkpoint ever N epochs\n  IMAGE_SIZE: 256               #Frames get resized to NxN\n\n#EfficientTrack 2D Keypoint Detector Configuration\nKEYPOINTDETECT:\n  MODEL_SIZE: 'small'           #Can be 'small', 'medium' or 'large'\n  BATCH_SIZE: 8                 #Set to 4 for very small datasets (&lt;500 Frames) \n  MAX_LEARNING_RATE: 0.01       #Max learning rate in OneCycle schedule\n  NUM_EPOCHS: 100               #Set to 200 for very small datasets\n  CHECKPOINT_SAVE_INTERVAL: 10  #Saves a .pth checkpoint ever N epochs\n  BOUNDING_BOX_SIZE: 256        #Size of the crop around the subject that gets \n                                # fed into KeypointDetect (1)\n  NUM_JOINTS: 23                #Number of keypoints (Don't change!)\n\n#hybridNet Configuration\nHYBRIDNET:\n  BATCH_SIZE: 1                 #Currently only batch size 1 is supported\n  MAX_LEARNING_RATE: 0.003      #Max learning rate in OneCycle schedule\n  NUM_EPOCHS: 30                #Set to 60 for very small datasets (&lt;500 Frames)\n  CHECKPOINT_SAVE_INTERVAL: 10  #Saves a .pth checkpoint ever N epochs\n  NUM_CAMERAS: 12               #Number fo cameras (Don't change!)\n  ROI_CUBE_SIZE: 144            #Size of the 3D bounding box in mm (2)\n  GRID_SPACING: 2               #Resolution of the 3D bounding box in mm \n\nKEYPOINT_NAMES:                 #List of all keypoint names (for visualization)\n- Pinky_T\n- Pinky_D\n...\n\nSKELETON:                   #List of all joints (for visualization only)\n- - Pinky_T\n  - Pinky_D\n- - Pinky_D\n  - Pinky_M\n- - Pinky_M\n...\n</code></pre>"},{"location":"model_database/model_database/","title":"Pretrained Model Database","text":"<p>Welcome to our Model Database! Here you will find all of the pretrained HybridNet Models currently available for download.\\ All the models here are officially supported and we only list models that achieve a high level of accuracy. Please note that these models should be used for pretraining only and you still need to train the network on a dataset created for your specific setup. To include them in your JARVIS installation simply extract them into the pretrained folder inside your main JARVIS-HybridNet directory.  Along with each set of models we provide the trainingset that the model was trained with as well as set of recordings for you to validate the models performance with.</p> Monkey Hand Annotated Frames 3000 Number of subjects 1 Dataset Size 0.7 GB Recordings Size 1.3 GB Download Models Download Trainingset Download Recording Human Hand Annotated Frames 3000 Number of subjects 4 Dataset Size 1.5 GB Recordings Size Download Models Coming Soon Coming Soon Rat Full Body Annotated Frames 2000 Number of subjects 1 Dataset Size 1.5 GB Recordings Size 1.3 GB Download Models Coming Soon Coming Soon Mouse Full Body Annotated Frames 3000 Number of subjects 3 Dataset Size 1.5 GB Recordings Size 1.3 GB Download Models Coming Soon Coming Soon"}]}